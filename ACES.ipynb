{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsJu8PoozaePpj/qb68SLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/case/blob/main/ACES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you get an error like:\n",
        "\n",
        "`Transport endpoint is not connected`\n",
        "\n",
        "Goto Runtime on menubar and click on restart runtime option"
      ],
      "metadata": {
        "id": "viH7qjLSlnWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to my google drive which is where ive stored the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/case\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuiwbD98XEyf",
        "outputId": "3b415f0b-5b78-4a8e-a691-d938aaf939bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/case\n",
            "ACES.ipynb  modelstates  sol.ipynb\t\t\t   utils.py\n",
            "data\t    __pycache__  T0_3B_dialog_summarization.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "# !wget https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
        "# !tar -xvf empatheticdialogues.tar.gz\n",
        "# !rm empatheticdialogues.tar.gz\n",
        "\n",
        "path_to_empatheticdialogues = 'data/empatheticdialogues/'\n",
        "data_split_list = os.listdir(path_to_empatheticdialogues)\n",
        "print(data_split_list)\n",
        "\n",
        "\"\"\"\n",
        "convo_list is a list of utterance strings whose length is the total\n",
        "number of back and forth exchanges between 2 speakers. convo_list_of_lists\n",
        "is a list of convo_list's. \n",
        "\"\"\"\n",
        "convo_list_of_lists = [] # accumulates all convos\n",
        "convo_list = [] # emptied after each convo\n",
        "convo_lengths = [] #to gather data stats\n",
        "utterance_lengths = [] #to gather data stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XB4K13cpY3I",
        "outputId": "62abe202-f3af-4a3f-f5fc-20924ab9c12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test.csv', 'train.csv', 'valid.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitname = \"train\"#\"test\"#\"valid\"#\n",
        "\n",
        "df = open(os.path.join(path_to_empatheticdialogues, f\"{splitname}.csv\")).readlines()\n",
        "\n",
        "# set this to True to see what is happening\n",
        "verbose = False\n",
        "# set this to true to include each pair of back and forth utrerances in the data\n",
        "# in addition to the single multi-turn conversation\n",
        "add_utter_pairs = False \n",
        "\n",
        "if verbose:\n",
        "    print(df[0].strip().split(\",\"))\n",
        "    print(\"________________________________\")\n",
        "\n",
        "for i in range(1, len(df)):\n",
        "    \n",
        "    cparts = df[i - 1].strip().split(\",\")\n",
        "    sparts = df[i].strip().split(\",\")\n",
        "    prevsent = cparts[5].replace(\"_comma_\", \",\")\n",
        "    \n",
        "    utter_idx = int(sparts[1])\n",
        "\n",
        "    \"\"\"\n",
        "    If the current and previous utterance have the same conv_id, or\n",
        "    conversation ID, then they are part of the same conversation and\n",
        "    we will append previous utterance it to convo_list and go to the next line, \n",
        "    otherwise we append the utterance to the convo_list as the last utterance\n",
        "    append the whole conversation to our list of converstations and refresh the\n",
        "    convo_list\n",
        "    \"\"\"\n",
        "    if (cparts[0] == sparts[0]):\n",
        "        \n",
        "        convo_list.append(prevsent)\n",
        "        utterance_lengths.append(len(prevsent))\n",
        "        \n",
        "        if verbose:\n",
        "            print(cparts)\n",
        "            print(sparts)\n",
        "            print(\" \")\n",
        "            print(prevsent)\n",
        "            print(\" \")\n",
        "            \n",
        "        if add_utter_pairs & ((utter_idx % 2) == 0):\n",
        "            convo_list_of_lists.append(\n",
        "                [\n",
        "                    cparts[5].replace(\"_comma_\", \",\"),\n",
        "                    sparts[5].replace(\"_comma_\", \",\")\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    elif i > 1:\n",
        "        \n",
        "        convo_list.append(prevsent)\n",
        "        utterance_lengths.append(len(prevsent))\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"LAST_UTTERSANCE\")\n",
        "            print(\" \")\n",
        "            print(prevsent)\n",
        "            print(\" \")\n",
        "            print(\"__________________________________\")\n",
        "        \n",
        "            if i > 20:\n",
        "                break\n",
        "                \n",
        "        convo_len = len(convo_list)\n",
        "        utterance_lens = utterance_lengths[-convo_len:]\n",
        "        \n",
        "        if (convo_len > 1):\n",
        "            convo_list_of_lists.append(convo_list)\n",
        "            convo_lengths.append(len(convo_list))\n",
        "        \n",
        "        # at the end of a conversation, clear the convo_list for the next convo\n",
        "        convo_list = []\n",
        "     \n",
        "    \n",
        "if verbose:\n",
        "    print(convo_list_of_lists)\n",
        "    \n",
        "print('number of convos', len(convo_list_of_lists))\n",
        "print('min, max, mean turns', min(convo_lengths), max(convo_lengths), np.mean(convo_lengths))\n",
        "print('min, max, mean utter lengths', min(utterance_lengths), max(utterance_lengths), np.mean(utterance_lengths))\n",
        "# test 2546\n",
        "# test + validation 5314\n",
        "# test + validation + train 24844\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeH4TBWvbJxG",
        "outputId": "48f457e9-3449-41ed-ad53-458728de15e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of convos 24844\n",
            "min, max, mean turns 2 9 4.315367895668975\n",
            "min, max, mean utter lengths 1 640 67.9634852681944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the extracted list of convos as a json file\n",
        "'''\n",
        "with open('data/convo_list_of_lists.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(convo_list_of_lists, f, ensure_ascii=False, indent=4)\n",
        "'''\n",
        "\n",
        "with open('data/convo_list_of_lists.json') as f:\n",
        "    convo_list_of_lists = json.load(f)\n",
        "\n",
        "convo_list_of_lists[-1]"
      ],
      "metadata": {
        "id": "DbsuiCZihYUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9473f25-cae9-41a7-ef7b-9141015e2032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was going through the stuff in my attic last night',\n",
              " 'Did you find anything great?',\n",
              " \"Yeah I found some old pictures of when us kids used to go to my grandma's house for xmas\",\n",
              " 'What a wonderful memory.  ',\n",
              " 'Yeah reminds me of the good old days.  I miss my grandma.  She passed away about 15 years ago. ']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Conversationnel Emotionnel Social (ACES)\n",
        "\n",
        "Large Causal Language Models, also known as autoregresive models, make excllent chatbots because they are not only trained to predict the next tokens in dialog text but have also pretrained on the much larger body of data on the internet aside from conversation. \n",
        "\n",
        "But useful agents cannot simply reply with the most likely response from it's training data. There needs to be a self awareness or self monitoring and the human controller should be able to update the behavior of the agent.\n",
        "\n",
        "\n",
        "Some strategies might include: training and evaluating on few shot monitoring goals, reinforcement learning, GANs to maintain conversational realism, planning and explaining\n"
      ],
      "metadata": {
        "id": "mHxUAMVEkWTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wGYtvyrGVf6L"
      },
      "outputs": [],
      "source": [
        "# load dependencies into our python environment\n",
        "%%capture\n",
        "! pip install transformers[sentencepiece] datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#plotting tools\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print('transformers.__version__', transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNgpdxj7V-gg",
        "outputId": "ab128e4e-9d61-42db-8977-f272f450e9c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.11.0+cu113\n",
            "torch.cuda.device_count() 1\n",
            "torch.cuda.empty_cache() None\n",
            "transformers.__version__ 4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_model = 'gpt2'):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if pretrained_model in ['gpt2']:\n",
        "            \n",
        "            cache_dir = os.path.join(\n",
        "                \"./modelstates/hugface_models/\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"cache_dir=\", cache_dir)\n",
        "            \n",
        "            model_save_path = os.path.join(\n",
        "                \"./modelstates/finetuned_models\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"model_save_path=\", model_save_path)\n",
        "            \n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "                pretrained_model,\n",
        "                pad_token='<|endoftext|>',\n",
        "            )\n",
        "            \n",
        "            self.model = GPT2LMHeadModel.from_pretrained(\n",
        "                pretrained_model,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=0.00005,\n",
        "            betas=(0.9, 0.98),\n",
        "            eps=1e-9,\n",
        "        )\n",
        "        \n",
        "        self.num_gpus = torch.cuda.device_count()\n",
        "        \n",
        "        if self.num_gpus > 1:\n",
        "            self.model.parallelize()\n",
        "        elif self.num_gpus == 1:\n",
        "            self.gpu0 = torch.device('cuda:0')\n",
        "            #self.model = self.model.cuda()\n",
        "            self.model = self.model.to(self.gpu0)\n",
        "            '''you can do .to(cuda0) with tensors to'''\n",
        "            \n",
        "        print('self.model.device', self.model.device)\n",
        "        \n",
        "        self.num_params = \\\n",
        "          sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"num_params\", self.num_params)\n",
        "        \n",
        "    def get_response(self, prompt, max_len = 32, no_repeat_ngram_size = None):\n",
        "        \n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        \n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model.device)\n",
        "            prompt_mask = prompt_mask.to(self.model.device)\n",
        "        \n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        \n",
        "        output_ids = self.model.generate(\n",
        "            prompt_ids,\n",
        "            attention_mask = prompt_mask,\n",
        "            max_length = prompt_len+max_len,\n",
        "            no_repeat_ngram_size = no_repeat_ngram_size,\n",
        "        )\n",
        "\n",
        "        generated_text = self.tokenizer.batch_decode(output_ids)[0]\n",
        "        \n",
        "        return generated_text\n",
        "    \n",
        "    def memorize(self, prompt, num_epochs = 3):\n",
        "\n",
        "        print('start training loop')\n",
        "\n",
        "        \"\"\" This is a rudimentary training loop\n",
        "        that will train the agent to learn one\n",
        "        sequence, the prompt. With enough epochs, this should\n",
        "        result in memorizing the sequence, which is why this\n",
        "        class method was names memorize. \n",
        "        There is nothing returned because the model attribute is modified inplace. \n",
        "        Args:\n",
        "            prompt (string): the text to be learned\n",
        "            num_epochs (int): the number of times we cycle though the training data, only 1 sample in this case\n",
        "        \"\"\"\n",
        "\n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model.device)\n",
        "            prompt_mask = prompt_mask.to(self.model.device)\n",
        "            \n",
        "        source_ids = prompt_ids[:,:-1]\n",
        "        target_ids = prompt_ids[:,1:]\n",
        "        source_mask = prompt_mask[:,:-1]\n",
        "        target_mask = prompt_mask[:,1:]\n",
        "\n",
        "        # allow params to be updated\n",
        "        self.model.train()\n",
        "\n",
        "        for e in range(num_epochs):\n",
        "\n",
        "            # Forward Pass To Loss\n",
        "            output = self.model(\n",
        "                input_ids = source_ids,\n",
        "                attention_mask = source_mask,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            logits = output.logits\n",
        "\n",
        "            loss = cross_entropy_loss(\n",
        "                logits, \n",
        "                target_ids, \n",
        "            )\n",
        "\n",
        "            '''\n",
        "            # Equally valid way to do Forward Pass To Loss\n",
        "            # labels are automatically shifted into targets\n",
        "            outputs = self.model(\n",
        "                input_ids = prompt_ids,\n",
        "                labels = prompt_ids,\n",
        "                attention_mask = prompt_mask,\n",
        "                token_type_ids=None,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            '''\n",
        "\n",
        "            # backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(\"epoch\", e, \"loss\", loss.item())\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def cross_entropy_loss(logits, target_ids):\n",
        "    \n",
        "    \"\"\"\n",
        "    For F.cross_entropy the Input is shape (N, C), where N = batch_size x sequence_length\n",
        "    and C is the number of classes, in our case C is the number of tokens in the vocabulary\n",
        "    Target is shape (N).\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "    we flatten the batch dimension together with the max_seq length\n",
        "    so that for the loss funstion, so afterwards, there is no batch dimension,\n",
        "    just a vector sized C-dimensions for each of the seq_len tokens. \n",
        "    If there had been 2 sampels with a batch size of 2, with 3 tokens in each sample\n",
        "    then the predictions.shape would be torch.Size([6, 50257])\n",
        "    Args:\n",
        "        logits (torch.tensor, float): shape [batch_size, sequence_length, vocab_size]\n",
        "        target_ids (torch.tensor, int): shape [batch_size, sequence_length]\n",
        "    Returns: \n",
        "        scalar_loss (torch.tensor, scalar float, grad_fn=<NllLossBackward0>)): no shape\n",
        "            this is a loss you can backpropagate using:\n",
        "            optimizer.zero_grad()\n",
        "            scalar_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "    \"\"\"\n",
        "    \n",
        "    predictions = logits.view(-1, logits.size(-1))\n",
        "    target = target_ids.view(-1)\n",
        "\n",
        "    scalar_loss = F.cross_entropy(\n",
        "        predictions,\n",
        "        target,\n",
        "    )\n",
        "\n",
        "    return scalar_loss\n",
        "\n",
        "# place the model and tokenizer into our dialog agent\n",
        "\n",
        "agent = BaseAgent(\n",
        "    pretrained_model = 'gpt2'\n",
        ")"
      ],
      "metadata": {
        "id": "TXkyOg74ie4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9974021-dd4f-4465-b88c-8596c68afb4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_dir= ./modelstates/hugface_models/gpt2\n",
            "model_save_path= ./modelstates/finetuned_models/gpt2\n",
            "self.model.device cuda:0\n",
            "num_params 124439808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changes in Model Behavior due to Training\n",
        "\n",
        "The point of all these examples below before and after the training is that with enough training the model will memorize the entire sequences given to it from the training data\n"
      ],
      "metadata": {
        "id": "BvKz9f-H-ww0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal Prompt\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB:\",\n",
        "    max_len = 32,\n",
        "    no_repeat_ngram_size = 4,\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRbAS2ac-O2p",
        "outputId": "a056be81-090b-4409-dfc7-3fa810283a6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello.\n",
            "B: I'm sorry.\n",
            "C: I'm not sure what you're talking about.\n",
            "D: I'm just wondering if you're aware of the fact that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly longer more specific prompt\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi there.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 32,\n",
        "    no_repeat_ngram_size = 4,\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcQbD7jx-bGP",
        "outputId": "fcaae533-75fd-401d-bb89-bca7a9aefd72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello.\n",
            "B: Hi there.\n",
            "A: How was your day?\n",
            "B: I was in the hospital.\n",
            "A. How did you get there?\n",
            "B. I was in a hospital.\n",
            "B. How did your day go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of how to learn to extend in a directed manner\n",
        "\n",
        "agent.memorize(\n",
        "    \"A: Hello.\\nB: Hi there.\\nA: How was your day?\\nB: First, you tell me about your day.\\nA: Well its a long story.\",\n",
        "    num_epochs = 20,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jGu9HtcaWu1",
        "outputId": "e79f4eb0-2388-4377-de56-56790a30c035"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training loop\n",
            "epoch 0 loss 2.6466197967529297\n",
            "epoch 1 loss 2.1712193489074707\n",
            "epoch 2 loss 1.6314619779586792\n",
            "epoch 3 loss 1.2606326341629028\n",
            "epoch 4 loss 0.9254641532897949\n",
            "epoch 5 loss 0.8931131958961487\n",
            "epoch 6 loss 0.6917286515235901\n",
            "epoch 7 loss 0.44015634059906006\n",
            "epoch 8 loss 0.5122154355049133\n",
            "epoch 9 loss 0.26708248257637024\n",
            "epoch 10 loss 0.34200185537338257\n",
            "epoch 11 loss 0.39959222078323364\n",
            "epoch 12 loss 0.0948568806052208\n",
            "epoch 13 loss 0.17605099081993103\n",
            "epoch 14 loss 0.06733689457178116\n",
            "epoch 15 loss 0.03075048141181469\n",
            "epoch 16 loss 0.013989539816975594\n",
            "epoch 17 loss 0.017403578385710716\n",
            "epoch 18 loss 0.04515552148222923\n",
            "epoch 19 loss 0.12899145483970642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal Prompt\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB:\",\n",
        "    max_len = 32,\n",
        "    no_repeat_ngram_size = 4,\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "XP1K1DYZlX2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39c36ed-5970-47f2-c7a0-01e527682c91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello.\n",
            "B: Hi there.\n",
            "A: How was your day?\n",
            "B: First, you tell me about your day.\n",
            "A. Well its a long story.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly longer more specific prompt\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi there.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 32,\n",
        "    no_repeat_ngram_size = 4,\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "rmvz0ITjnkKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd89487e-82c2-4c0d-a00a-d0979dd85b52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello.\n",
            "B: Hi there.\n",
            "A: How was your day?\n",
            "B: First, you tell me about your day.\n",
            "A. Well its a long story.\n",
            "B. Well its an easy story.\n",
            "A.: Well its\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tzBArhBfCZmI"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}