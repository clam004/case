{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4WqpO+GHMKlGLzLuGVVYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/case/blob/main/ACES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wGYtvyrGVf6L"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers[sentencepiece] datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/case\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuiwbD98XEyf",
        "outputId": "fa161301-8129-4948-85ca-dfeff465c0e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/case\n",
            "ACES.ipynb\t     modelstates  T0_3B_dialog_summarization.ipynb\n",
            "empatheticdialogues  __pycache__  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#plotting tools\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print('transformers.__version__', transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNgpdxj7V-gg",
        "outputId": "c60c988b-5c72-4965-b1fa-111da427db8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.11.0+cu113\n",
            "torch.cuda.device_count() 1\n",
            "torch.cuda.empty_cache() None\n",
            "transformers.__version__ 4.20.1\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_model = 'gpt2'):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if pretrained_model in ['gpt2']:\n",
        "            \n",
        "            cache_dir = os.path.join(\n",
        "                \"./modelstates/hugface_models/\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"cache_dir=\", cache_dir)\n",
        "            \n",
        "            model_save_path = os.path.join(\n",
        "                \"./modelstates/finetuned_models\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"model_save_path=\", model_save_path)\n",
        "            \n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "                pretrained_model,\n",
        "                pad_token='<|endoftext|>',\n",
        "            )\n",
        "            \n",
        "            self.model = GPT2LMHeadModel.from_pretrained(\n",
        "                pretrained_model,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=0.00005,\n",
        "            betas=(0.9, 0.98),\n",
        "            eps=1e-9,\n",
        "        )\n",
        "        \n",
        "        self.num_gpus = torch.cuda.device_count()\n",
        "        \n",
        "        if self.num_gpus > 1:\n",
        "            self.model.parallelize()\n",
        "        elif self.num_gpus == 1:\n",
        "            self.gpu0 = torch.device('cuda:0')\n",
        "            #self.model = self.model.cuda()\n",
        "            self.model = self.model.to(self.gpu0)\n",
        "            '''you can do .to(cuda0) with tensors to'''\n",
        "            \n",
        "        self.model_device = next(self.model.parameters()).device\n",
        "        print('model_device', self.model_device)\n",
        "        \n",
        "        self.num_params = \\\n",
        "          sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"num_params\", self.num_params)\n",
        "        \n",
        "    def get_response(self, prompt, max_len = 32):\n",
        "        \n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "        \n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "        \n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        \n",
        "        output_ids = self.model.generate(\n",
        "            prompt_ids,\n",
        "            attention_mask = prompt_mask,\n",
        "            max_length=prompt_len+max_len,\n",
        "        )\n",
        "\n",
        "        generated_text = self.tokenizer.batch_decode(output_ids)[0]\n",
        "        \n",
        "        return generated_text\n",
        "    \n",
        "    def memorize(self, prompt, num_epochs = 3):\n",
        "\n",
        "        print('start training loop')\n",
        "\n",
        "        \"\"\" This is a rudimentary training loop\n",
        "        that will train the agent to learn one\n",
        "        sequence, the prompt. With enough epochs, this should\n",
        "        result in memorizing the sequence, which is why this\n",
        "        class method was names memorize. \n",
        "        There is nothing returned because the model attribute is modified inplace. \n",
        "        Args:\n",
        "            prompt (string): the text to be learned\n",
        "            num_epochs (int): the number of times we cycle though the training data, only 1 sample in this case\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        \n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "            \n",
        "        source_ids = prompt_ids[:,:-1]\n",
        "        target_ids = prompt_ids[:,1:]\n",
        "        source_mask = prompt_mask[:,:-1]\n",
        "        target_mask = prompt_mask[:,1:]\n",
        "\n",
        "        # allow params to be updated\n",
        "        self.model.train()\n",
        "\n",
        "        for e in range(num_epochs):\n",
        "\n",
        "            # Forward Pass To Loss\n",
        "            output = self.model(\n",
        "                input_ids = source_ids,\n",
        "                attention_mask = source_mask,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            logits = output.logits\n",
        "\n",
        "            loss = cross_entropy_loss(\n",
        "                logits, \n",
        "                target_ids, \n",
        "            )\n",
        "\n",
        "            '''\n",
        "            # Equally valid way to do Forward Pass To Loss\n",
        "            # labels are automatically shifted into targets\n",
        "            outputs = self.model(\n",
        "                input_ids = prompt_ids,\n",
        "                labels = prompt_ids,\n",
        "                attention_mask = prompt_mask,\n",
        "                token_type_ids=None,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            '''\n",
        "\n",
        "            # backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(\"epoch\", e, \"loss\", loss.item())\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def cross_entropy_loss(logits, target_ids):\n",
        "    \n",
        "    \"\"\"\n",
        "    For F.cross_entropy the Input is shape (N, C), where N = batch_size x sequence_length\n",
        "    and C is the number of classes, in our case C is the number of tokens in the vocabulary\n",
        "    Target is shape (N).\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "    we flatten the batch dimension together with the max_seq length\n",
        "    so that for the loss funstion, so afterwards, there is no batch dimension,\n",
        "    just a vector sized C-dimensions for each of the seq_len tokens. \n",
        "    If there had been 2 sampels with a batch size of 2, with 3 tokens in each sample\n",
        "    then the predictions.shape would be torch.Size([6, 50257])\n",
        "    Args:\n",
        "        logits (torch.tensor, float): shape [batch_size, sequence_length, vocab_size]\n",
        "        target_ids (torch.tensor, int): shape [batch_size, sequence_length]\n",
        "    Returns: \n",
        "        scalar_loss (torch.tensor, scalar float, grad_fn=<NllLossBackward0>)): no shape\n",
        "            this is a loss you can backpropagate using:\n",
        "            optimizer.zero_grad()\n",
        "            scalar_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "    \"\"\"\n",
        "    \n",
        "    predictions = logits.view(-1, logits.size(-1))\n",
        "    target = target_ids.view(-1)\n",
        "\n",
        "    scalar_loss = F.cross_entropy(\n",
        "        predictions,\n",
        "        target,\n",
        "    )\n",
        "\n",
        "    return scalar_loss\n",
        "\n",
        "# place the model and tokenizer into our dialog agent\n",
        "\n",
        "agent = BaseAgent(\n",
        "    pretrained_model = 'gpt2'\n",
        ")"
      ],
      "metadata": {
        "id": "TXkyOg74ie4j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Conversationnel Emotionnel Social (ACES)\n",
        "\n",
        "Large Causal Language Models, also known as autoregresive models, make excllent chatbots because they are not only trained to predict the next tokens in dialog text but have also pretrained on the much larger body of data on the internet aside from conversation. \n",
        "\n",
        "But useful agents cannot simply reply with the most likely response from it's training data. There needs to be a self awareness or self monitoring and the human controller should be able to update the behavior of the agent.\n",
        "\n",
        "\n",
        "Some strategies  might include: training and evaluating on few shot monitoring goals, GANs to maintain conversational realism, planning and explaining\n"
      ],
      "metadata": {
        "id": "mHxUAMVEkWTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# an example of how the pretrained model can extend dialog\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 16\n",
        ")\n",
        "\n",
        "print(generated_text)\n",
        "\n",
        "# an example of how to learn to extend in a directed manner\n",
        "\n",
        "agent.memorize(\n",
        "    \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB: First, you tell me about your day.\",\n",
        "    num_epochs = 5,\n",
        ")\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 16\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jGu9HtcaWu1",
        "outputId": "7f9867e8-e1e4-46e5-f3c0-ffad56c7450c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_dir= ./modelstates/hugface_models/gpt2\n",
            "model_save_path= ./modelstates/finetuned_models/gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_device cuda:0\n",
            "num_params 124439808\n",
            "A: Hello.\n",
            "B: Hi.\n",
            "A: How was your day?\n",
            "B: I was in the hospital.\n",
            "A: What was your name?\n",
            "B\n",
            "start training loop\n",
            "epoch 0 loss 2.6564619541168213\n",
            "epoch 1 loss 2.4013028144836426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 loss 1.9447654485702515\n",
            "epoch 3 loss 1.4950447082519531\n",
            "epoch 4 loss 0.9667388200759888\n",
            "A: Hello.\n",
            "B: Hi.\n",
            "A: How was your day?\n",
            "B: Well, I was just sitting in my room, and I was thinking about the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
        "# !tar -xvf empatheticdialogues.tar.gz\n",
        "# !rm empatheticdialogues.tar.gz\n",
        "path_to_empatheticdialogues = 'empatheticdialogues/'\n",
        "data_split_list = os.listdir(path_to_empatheticdialogues)\n",
        "print(data_split_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XB4K13cpY3I",
        "outputId": "02db0483-5d33-4443-b950-d91f17cf8628"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test.csv', 'train.csv', 'valid.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zcQPNdi0ryZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}