{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyrCG1Xn1SdJJ1gM9CTluT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam004/case/blob/main/ACES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wGYtvyrGVf6L"
      },
      "outputs": [],
      "source": [
        "# load dependencies into our python environment\n",
        "%%capture\n",
        "! pip install transformers[sentencepiece] datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to my google drive which is where ive stored the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/case\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuiwbD98XEyf",
        "outputId": "5355c0e0-166f-4698-a7a4-28a4e39273db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/case'\n",
            "/content/drive/MyDrive/case\n",
            "ACES.ipynb\t     modelstates  T0_3B_dialog_summarization.ipynb\n",
            "empatheticdialogues  __pycache__  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "\n",
        "#data manupulation libs\n",
        "import numpy as np\n",
        "\n",
        "#plotting tools\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "print('torch.__version__', torch.__version__)\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print('torch.cuda.device_count()', torch.cuda.device_count())\n",
        "print('torch.cuda.empty_cache()', torch.cuda.empty_cache())\n",
        "\n",
        "#huggingface transformers\n",
        "import transformers\n",
        "print('transformers.__version__', transformers.__version__)\n",
        "from transformers import set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# seeds\n",
        "set_seed(42)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNgpdxj7V-gg",
        "outputId": "909fb611-cdea-4099-cfb6-6202a464385f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.__version__ 1.11.0+cu113\n",
            "torch.cuda.device_count() 1\n",
            "torch.cuda.empty_cache() None\n",
            "transformers.__version__ 4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_model = 'gpt2'):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if pretrained_model in ['gpt2']:\n",
        "            \n",
        "            cache_dir = os.path.join(\n",
        "                \"./modelstates/hugface_models/\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"cache_dir=\", cache_dir)\n",
        "            \n",
        "            model_save_path = os.path.join(\n",
        "                \"./modelstates/finetuned_models\",\n",
        "                pretrained_model,\n",
        "            )\n",
        "            \n",
        "            print(\"model_save_path=\", model_save_path)\n",
        "            \n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "                pretrained_model,\n",
        "                pad_token='<|endoftext|>',\n",
        "            )\n",
        "            \n",
        "            self.model = GPT2LMHeadModel.from_pretrained(\n",
        "                pretrained_model,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=0.00005,\n",
        "            betas=(0.9, 0.98),\n",
        "            eps=1e-9,\n",
        "        )\n",
        "        \n",
        "        self.num_gpus = torch.cuda.device_count()\n",
        "        \n",
        "        if self.num_gpus > 1:\n",
        "            self.model.parallelize()\n",
        "        elif self.num_gpus == 1:\n",
        "            self.gpu0 = torch.device('cuda:0')\n",
        "            #self.model = self.model.cuda()\n",
        "            self.model = self.model.to(self.gpu0)\n",
        "            '''you can do .to(cuda0) with tensors to'''\n",
        "            \n",
        "        self.model_device = next(self.model.parameters()).device\n",
        "        print('model_device', self.model_device)\n",
        "        \n",
        "        self.num_params = \\\n",
        "          sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"num_params\", self.num_params)\n",
        "        \n",
        "    def get_response(self, prompt, max_len = 32):\n",
        "        \n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "        \n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "        \n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        \n",
        "        output_ids = self.model.generate(\n",
        "            prompt_ids,\n",
        "            attention_mask = prompt_mask,\n",
        "            max_length=prompt_len+max_len,\n",
        "        )\n",
        "\n",
        "        generated_text = self.tokenizer.batch_decode(output_ids)[0]\n",
        "        \n",
        "        return generated_text\n",
        "    \n",
        "    def memorize(self, prompt, num_epochs = 3):\n",
        "\n",
        "        print('start training loop')\n",
        "\n",
        "        \"\"\" This is a rudimentary training loop\n",
        "        that will train the agent to learn one\n",
        "        sequence, the prompt. With enough epochs, this should\n",
        "        result in memorizing the sequence, which is why this\n",
        "        class method was names memorize. \n",
        "        There is nothing returned because the model attribute is modified inplace. \n",
        "        Args:\n",
        "            prompt (string): the text to be learned\n",
        "            num_epochs (int): the number of times we cycle though the training data, only 1 sample in this case\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        \n",
        "        prompt_dic = self.tokenizer(prompt,return_tensors=\"pt\")\n",
        "        prompt_ids = prompt_dic.input_ids\n",
        "        prompt_mask = prompt_dic.attention_mask\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        if self.num_gpus > 0:\n",
        "            prompt_ids = prompt_ids.to(self.model_device)\n",
        "            prompt_mask = prompt_mask.to(self.model_device)\n",
        "            \n",
        "        source_ids = prompt_ids[:,:-1]\n",
        "        target_ids = prompt_ids[:,1:]\n",
        "        source_mask = prompt_mask[:,:-1]\n",
        "        target_mask = prompt_mask[:,1:]\n",
        "\n",
        "        # allow params to be updated\n",
        "        self.model.train()\n",
        "\n",
        "        for e in range(num_epochs):\n",
        "\n",
        "            # Forward Pass To Loss\n",
        "            output = self.model(\n",
        "                input_ids = source_ids,\n",
        "                attention_mask = source_mask,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            logits = output.logits\n",
        "\n",
        "            loss = cross_entropy_loss(\n",
        "                logits, \n",
        "                target_ids, \n",
        "            )\n",
        "\n",
        "            '''\n",
        "            # Equally valid way to do Forward Pass To Loss\n",
        "            # labels are automatically shifted into targets\n",
        "            outputs = self.model(\n",
        "                input_ids = prompt_ids,\n",
        "                labels = prompt_ids,\n",
        "                attention_mask = prompt_mask,\n",
        "                token_type_ids=None,\n",
        "            )\n",
        "\n",
        "            # used logits and target tokens to calculate the loss\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            '''\n",
        "\n",
        "            # backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(\"epoch\", e, \"loss\", loss.item())\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def cross_entropy_loss(logits, target_ids):\n",
        "    \n",
        "    \"\"\"\n",
        "    For F.cross_entropy the Input is shape (N, C), where N = batch_size x sequence_length\n",
        "    and C is the number of classes, in our case C is the number of tokens in the vocabulary\n",
        "    Target is shape (N).\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "    we flatten the batch dimension together with the max_seq length\n",
        "    so that for the loss funstion, so afterwards, there is no batch dimension,\n",
        "    just a vector sized C-dimensions for each of the seq_len tokens. \n",
        "    If there had been 2 sampels with a batch size of 2, with 3 tokens in each sample\n",
        "    then the predictions.shape would be torch.Size([6, 50257])\n",
        "    Args:\n",
        "        logits (torch.tensor, float): shape [batch_size, sequence_length, vocab_size]\n",
        "        target_ids (torch.tensor, int): shape [batch_size, sequence_length]\n",
        "    Returns: \n",
        "        scalar_loss (torch.tensor, scalar float, grad_fn=<NllLossBackward0>)): no shape\n",
        "            this is a loss you can backpropagate using:\n",
        "            optimizer.zero_grad()\n",
        "            scalar_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "    \"\"\"\n",
        "    \n",
        "    predictions = logits.view(-1, logits.size(-1))\n",
        "    target = target_ids.view(-1)\n",
        "\n",
        "    scalar_loss = F.cross_entropy(\n",
        "        predictions,\n",
        "        target,\n",
        "    )\n",
        "\n",
        "    return scalar_loss\n",
        "\n",
        "# place the model and tokenizer into our dialog agent\n",
        "\n",
        "agent = BaseAgent(\n",
        "    pretrained_model = 'gpt2'\n",
        ")"
      ],
      "metadata": {
        "id": "TXkyOg74ie4j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Conversationnel Emotionnel Social (ACES)\n",
        "\n",
        "Large Causal Language Models, also known as autoregresive models, make excllent chatbots because they are not only trained to predict the next tokens in dialog text but have also pretrained on the much larger body of data on the internet aside from conversation. \n",
        "\n",
        "But useful agents cannot simply reply with the most likely response from it's training data. There needs to be a self awareness or self monitoring and the human controller should be able to update the behavior of the agent.\n",
        "\n",
        "\n",
        "Some strategies  might include: training and evaluating on few shot monitoring goals, GANs to maintain conversational realism, planning and explaining\n"
      ],
      "metadata": {
        "id": "mHxUAMVEkWTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# an example of how the pretrained model can extend dialog\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 16\n",
        ")\n",
        "\n",
        "print(generated_text)\n",
        "\n",
        "# an example of how to learn to extend in a directed manner\n",
        "\n",
        "agent.memorize(\n",
        "    \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB: First, you tell me about your day.\",\n",
        "    num_epochs = 5,\n",
        ")\n",
        "\n",
        "generated_text = agent.get_response(\n",
        "    prompt = \"A: Hello.\\nB: Hi.\\nA: How was your day?\\nB:\",\n",
        "    max_len = 16\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jGu9HtcaWu1",
        "outputId": "7f9867e8-e1e4-46e5-f3c0-ffad56c7450c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_dir= ./modelstates/hugface_models/gpt2\n",
            "model_save_path= ./modelstates/finetuned_models/gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_device cuda:0\n",
            "num_params 124439808\n",
            "A: Hello.\n",
            "B: Hi.\n",
            "A: How was your day?\n",
            "B: I was in the hospital.\n",
            "A: What was your name?\n",
            "B\n",
            "start training loop\n",
            "epoch 0 loss 2.6564619541168213\n",
            "epoch 1 loss 2.4013028144836426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 loss 1.9447654485702515\n",
            "epoch 3 loss 1.4950447082519531\n",
            "epoch 4 loss 0.9667388200759888\n",
            "A: Hello.\n",
            "B: Hi.\n",
            "A: How was your day?\n",
            "B: Well, I was just sitting in my room, and I was thinking about the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
        "# !tar -xvf empatheticdialogues.tar.gz\n",
        "# !rm empatheticdialogues.tar.gz\n",
        "\n",
        "path_to_empatheticdialogues = 'data/empatheticdialogues/'\n",
        "data_split_list = os.listdir(path_to_empatheticdialogues)\n",
        "print(data_split_list)\n",
        "\n",
        "\"\"\"\n",
        "convo_list is a list of utterance strings whose length is the total\n",
        "number of back and forth exchanges between 2 speakers. convo_list_of_lists\n",
        "is a list of convo_list's. \n",
        "\"\"\"\n",
        "convo_list_of_lists = []\n",
        "convo_list = []\n",
        "convo_lengths = [] #to gather data stats\n",
        "utterance_lengths = [] #to gather data stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XB4K13cpY3I",
        "outputId": "138d06ee-e9d9-4c94-df1a-93a0fcae7a77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test.csv', 'train.csv', 'valid.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitname = \"train\"#\"test\"#\"valid\"#\n",
        "\n",
        "df = open(os.path.join(path_to_empatheticdialogues, f\"{splitname}.csv\")).readlines()"
      ],
      "metadata": {
        "id": "ivXle6Dyarht"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set this to True to see what is happening\n",
        "verbose = False\n",
        "# set this to true to include each pair of back and forth utrerances in the data\n",
        "# in addition to the single multi-turn conversation\n",
        "add_utter_pairs = False \n",
        "\n",
        "if verbose:\n",
        "    print(df[0].strip().split(\",\"))\n",
        "    print(\"________________________________\")\n",
        "\n",
        "for i in range(1, len(df)):\n",
        "    \n",
        "    cparts = df[i - 1].strip().split(\",\")\n",
        "    sparts = df[i].strip().split(\",\")\n",
        "    prevsent = cparts[5].replace(\"_comma_\", \",\")\n",
        "    \n",
        "    utter_idx = int(sparts[1])\n",
        "\n",
        "    \"\"\"\n",
        "    If the current and previous utterance have the same conv_id, or\n",
        "    conversation ID, then they are part of the same conversation and\n",
        "    we will append previous utterance it to convo_list and go to the next line, \n",
        "    otherwise we append the utterance to the convo_list as the last utterance\n",
        "    append the whole conversation to our list of converstations and refresh the\n",
        "    convo_list\n",
        "    \"\"\"\n",
        "    if (cparts[0] == sparts[0]):\n",
        "        \n",
        "        convo_list.append(prevsent)\n",
        "        utterance_lengths.append(len(prevsent))\n",
        "        \n",
        "        if verbose:\n",
        "            print(cparts)\n",
        "            print(sparts)\n",
        "            print(\" \")\n",
        "            print(prevsent)\n",
        "            print(\" \")\n",
        "            \n",
        "        if add_utter_pairs & ((utter_idx % 2) == 0):\n",
        "            convo_list_of_lists.append(\n",
        "                [\n",
        "                    cparts[5].replace(\"_comma_\", \",\"),\n",
        "                    sparts[5].replace(\"_comma_\", \",\")\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    elif i > 1:\n",
        "        \n",
        "        convo_list.append(prevsent)\n",
        "        utterance_lengths.append(len(prevsent))\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"LAST_UTTERSANCE\")\n",
        "            print(\" \")\n",
        "            print(prevsent)\n",
        "            print(\" \")\n",
        "            print(\"__________________________________\")\n",
        "        \n",
        "            if i > 20:\n",
        "                break\n",
        "                \n",
        "        convo_len = len(convo_list)\n",
        "        utterance_lens = utterance_lengths[-convo_len:]\n",
        "        \n",
        "        if (convo_len > 1):\n",
        "            convo_list_of_lists.append(convo_list)\n",
        "            convo_lengths.append(len(convo_list))\n",
        "        \n",
        "        # at the end of a conversation, clear the convo_list for the next convo\n",
        "        convo_list = []\n",
        "     \n",
        "    \n",
        "if verbose:\n",
        "    print(convo_list_of_lists)\n",
        "    \n",
        "print('number of convos', len(convo_list_of_lists))\n",
        "print('min, max, mean turns', min(convo_lengths), max(convo_lengths), np.mean(convo_lengths))\n",
        "print('min, max, mean utter lengths', min(utterance_lengths), max(utterance_lengths), np.mean(utterance_lengths))\n",
        "# test 2546\n",
        "# test + validation 5314"
      ],
      "metadata": {
        "id": "HeH4TBWvbJxG",
        "outputId": "48f457e9-3449-41ed-ad53-458728de15e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of convos 24844\n",
            "min, max, mean turns 2 9 4.315367895668975\n",
            "min, max, mean utter lengths 1 640 67.9634852681944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_list_of_lists[-1]"
      ],
      "metadata": {
        "id": "krg4tbnEdbYF",
        "outputId": "a8a83317-ba79-4bb0-e6f2-bcf0417de1a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was going through the stuff in my attic last night',\n",
              " 'Did you find anything great?',\n",
              " \"Yeah I found some old pictures of when us kids used to go to my grandma's house for xmas\",\n",
              " 'What a wonderful memory.  ',\n",
              " 'Yeah reminds me of the good old days.  I miss my grandma.  She passed away about 15 years ago. ']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/convo_list_of_lists.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(convo_list_of_lists, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "DbsuiCZihYUX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XP1K1DYZlX2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}